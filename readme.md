This repository contains implementations of two probabilistic regression methods applied to the Auto MPG dataset:

Gaussian Mixture Model (GMM)

Mixture Density Network (MDN)

Both methods were developed and analyzed as part of my MS Thesis work:

Amish Anand (2025). Uncertainty Estimation in Regression Using Additive Models and Gaussian Processes. Masterâ€™s Thesis, Department of Data Science and Engineering, IISER Bhopal.

ðŸ“Œ Gaussian Mixture Model (GMM) Framework
After obtaining the initial prediction from our base model (GP-NAM), the residual is computed as:

r(x)=yâˆ’ 
y
^
â€‹
  
GPNAM
â€‹
 (x)

We model the joint distribution of the inputs and residuals with a Gaussian mixture model:

p(x,r)= 
k=1
âˆ‘
K
â€‹
 Ï€ 
k
â€‹
 N([ 
x
r
â€‹
 ] 

â€‹
 Î¼ 
k
â€‹
 ,Î£ 
k
â€‹
 )
where each component k has:

Mixing weight Ï€ 
k
â€‹
 , with âˆ‘ 
k=1
K
â€‹
 Ï€ 
k
â€‹
 =1.

Mean vector Î¼ 
k
â€‹
 =[ 
Î¼ 
k
x
â€‹
 
Î¼ 
k
r
â€‹
 
â€‹
 ].

Covariance matrix partitioned as:

Î£ 
k
â€‹
 =[ 
Î£ 
k
xx
â€‹
 
Î£ 
k
rx
â€‹
 
â€‹
  
Î£ 
k
xr
â€‹
 
Î£ 
k
rr
â€‹
 
â€‹
 ]
The number of components K is selected using the Bayesian Information Criterion (BIC).

For a new input x 
âˆ—
 , the conditional distribution of residual r is computed using Gaussian conditioning:

Î¼ 
râˆ£x
k
â€‹
 =Î¼ 
k
r
â€‹
 +Î£ 
k
rx
â€‹
 (Î£ 
k
xx
â€‹
 ) 
âˆ’1
 (x 
âˆ—
 âˆ’Î¼ 
k
x
â€‹
 )
$$\sigma^2_{r|x}^k=\Sigma^{rr}_k-\Sigma^{rx}_k(\Sigma^{xx}_k)^{-1}\Sigma^{xr}_k$$
The responsibility for each component is:

h 
k
â€‹
 (x 
âˆ—
 )= 
âˆ‘ 
j=1
K
â€‹
 Ï€ 
j
â€‹
 N(x 
âˆ—
 âˆ£Î¼ 
j
x
â€‹
 ,Î£ 
j
xx
â€‹
 )
Ï€ 
k
â€‹
 N(x 
âˆ—
 âˆ£Î¼ 
k
x
â€‹
 ,Î£ 
k
xx
â€‹
 )
â€‹
 
The overall predicted residual and its variance are:

r
^
 (x 
âˆ—
 )= 
k=1
âˆ‘
K
â€‹
 h 
k
â€‹
 (x 
âˆ—
 )Î¼ 
râˆ£x
k
â€‹
 
$$\hat{\sigma}^2(x^*)=\sum_{k=1}^Kh_k(x^*)\Big[\sigma^2_{r|x}^k+(\mu_{r|x}^k-\hat{r}(x^*))^2\Big]$$
Thus, the final prediction is:

y
^
â€‹
 (x 
âˆ—
 )= 
y
^
â€‹
  
GPNAM
â€‹
 (x 
âˆ—
 )+ 
r
^
 (x 
âˆ—
 )
with predictive uncertainty quantified by:

Ïƒ
^
 (x 
âˆ—
 )= 
Ïƒ
^
  
2
 (x 
âˆ—
 )

â€‹
 
ðŸ“Œ Mixture Density Network (MDN) Framework
The MDN models the distribution of residuals using a neural network-based probabilistic formulation.

Residuals are defined as:

r=yâˆ’ 
y
^
â€‹
  
base
â€‹
 

MDN Structure
Hidden Layer: Uses nonlinear activations (ReLU) to capture complex patterns.

Output Layer: Outputs 3m values for a mixture of m Gaussians:

Means: Î¼ 
1
â€‹
 ,...,Î¼ 
m
â€‹
 .

Log-standard deviations:  
Ïƒ
~
  
1
â€‹
 ,..., 
Ïƒ
~
  
m
â€‹
 , with Ïƒ 
i
â€‹
 =exp( 
Ïƒ
~
  
i
â€‹
 ).

Mixing coefficients:

Î± 
i
â€‹
 = 
âˆ‘ 
j=1
m
â€‹
 exp(Î± 
j
âˆ—
â€‹
 )
exp(Î± 
i
âˆ—
â€‹
 )
â€‹
 , 
i
âˆ‘
â€‹
 Î± 
i
â€‹
 =1
The MDN models the conditional distribution:

p(râˆ£x)= 
i=1
âˆ‘
m
â€‹
 Î± 
i
â€‹
 N(r;Î¼ 
i
â€‹
 ,Ïƒ 
i
2
â€‹
 )
Expected Value & Variance
E[râˆ£x]= 
i=1
âˆ‘
m
â€‹
 Î± 
i
â€‹
 Î¼ 
i
â€‹
 
Var(râˆ£x)= 
i=1
âˆ‘
m
â€‹
 Î± 
i
â€‹
 (Ïƒ 
i
2
â€‹
 +Î¼ 
i
2
â€‹
 )âˆ’( 
i=1
âˆ‘
m
â€‹
 Î± 
i
â€‹
 Î¼ 
i
â€‹
 ) 
2
 
Final Prediction
y
^
â€‹
 = 
y
^
â€‹
  
base
â€‹
 +E[râˆ£x]
ðŸ“– Citation
If you use this work, please cite the thesis and foundational references:

Thesis

Amish Anand. Uncertainty Estimation in Regression Using Additive Models and Gaussian Processes. MS Thesis, IISER Bhopal, 2025.

Foundational References

Douglas A Reynolds et al. Gaussian mixture models. Encyclopedia of Biometrics, 741(659-663):3, 2009.

Alexander Fabisch. gmr: Gaussian mixture regression. JOSS, 6(62):3054, 2021.

Zoubin Ghahramani and Michael Jordan. Supervised learning from incomplete data via an EM approach. NeurIPS, 1993.

Christopher M Bishop. Mixture density networks. Technical Report, 1994.

Axel Brando. Mixture density networks (MDN) for distribution and uncertainty estimation. Masterâ€™s Thesis, 2017.

Axel Brando. Mixture density networks (MDN) for distribution and uncertainty estimation. GitHub repository, 2017.

Wei Zhang, Brian Barr, and John Paisley. Gaussian process neural additive models. arXiv:2402.12518, 2024.

ðŸ“¬ Contact
ðŸ“§ amish6202@gmail.com
