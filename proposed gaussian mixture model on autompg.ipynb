{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1/10 ===\n",
      "Epoch 100: Base Loss = 0.0179\n",
      "Epoch 200: Base Loss = 0.0179\n",
      "Epoch 300: Base Loss = 0.0179\n",
      "Optimal number of GMM components: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - RMSE: 0.0842, ECE: 0.0242, NLPD: -17.6828\n",
      "\n",
      "=== Experiment 2/10 ===\n",
      "Epoch 100: Base Loss = 0.0215\n",
      "Epoch 200: Base Loss = 0.0215\n",
      "Epoch 300: Base Loss = 0.0215\n",
      "Optimal number of GMM components: 4\n",
      "Run 2 - RMSE: 0.0870, ECE: 0.0136, NLPD: -9.7902\n",
      "\n",
      "=== Experiment 3/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0197\n",
      "Epoch 200: Base Loss = 0.0197\n",
      "Epoch 300: Base Loss = 0.0197\n",
      "Optimal number of GMM components: 5\n",
      "Run 3 - RMSE: 0.0904, ECE: 0.0277, NLPD: -12.6677\n",
      "\n",
      "=== Experiment 4/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0197\n",
      "Epoch 200: Base Loss = 0.0197\n",
      "Epoch 300: Base Loss = 0.0197\n",
      "Optimal number of GMM components: 5\n",
      "Run 4 - RMSE: 0.0608, ECE: 0.0137, NLPD: -14.4397\n",
      "\n",
      "=== Experiment 5/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0201\n",
      "Epoch 200: Base Loss = 0.0201\n",
      "Epoch 300: Base Loss = 0.0201\n",
      "Optimal number of GMM components: 5\n",
      "Run 5 - RMSE: 0.0738, ECE: 0.0189, NLPD: -14.3284\n",
      "\n",
      "=== Experiment 6/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0200\n",
      "Epoch 200: Base Loss = 0.0200\n",
      "Epoch 300: Base Loss = 0.0200\n",
      "Optimal number of GMM components: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 - RMSE: 0.0959, ECE: 0.0222, NLPD: -44.3566\n",
      "\n",
      "=== Experiment 7/10 ===\n",
      "Epoch 100: Base Loss = 0.0201\n",
      "Epoch 200: Base Loss = 0.0201\n",
      "Epoch 300: Base Loss = 0.0201\n",
      "Optimal number of GMM components: 5\n",
      "Run 7 - RMSE: 0.0936, ECE: 0.0118, NLPD: -16.4912\n",
      "\n",
      "=== Experiment 8/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0183\n",
      "Epoch 200: Base Loss = 0.0183\n",
      "Epoch 300: Base Loss = 0.0183\n",
      "Optimal number of GMM components: 5\n",
      "Run 8 - RMSE: 0.0839, ECE: 0.0218, NLPD: -11.3308\n",
      "\n",
      "=== Experiment 9/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Base Loss = 0.0198\n",
      "Epoch 200: Base Loss = 0.0198\n",
      "Epoch 300: Base Loss = 0.0198\n",
      "Optimal number of GMM components: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 - RMSE: 0.0775, ECE: 0.0162, NLPD: -12.5983\n",
      "\n",
      "=== Experiment 10/10 ===\n",
      "Epoch 100: Base Loss = 0.0219\n",
      "Epoch 200: Base Loss = 0.0219\n",
      "Epoch 300: Base Loss = 0.0219\n",
      "Optimal number of GMM components: 4\n",
      "Run 10 - RMSE: 0.1135, ECE: 0.0356, NLPD: -18.9052\n",
      "\n",
      "=== Final Results over 10 Experiments ===\n",
      "Test RMSE: Mean = 0.0861, Std = 0.0134\n",
      "Test ECE:  Mean = 0.0206, Std = 0.0070\n",
      "Test NLPD: Mean = -17.2591, Std = 9.4235\n",
      "GPUs available: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713894/3076141758.py:85: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_means[k] = cond_mean\n",
      "/tmp/ipykernel_713894/3076141758.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  comp_vars[k] = cond_var\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Force TensorFlow to run on CPU only and disable XLA devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gpflow as gpf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from gmr import GMM  # Ensure you have installed the gmr package\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# -------------------------Metrics-------------------------------------#\n",
    "# Here NLPD is defined as the (positive) negative log predictive density,\n",
    "# so smaller values are better (same convention as in Model 2).\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))).numpy()\n",
    "\n",
    "def ece(y_true, y_pred, n_bins=10):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_pred, bin_edges, right=True) - 1\n",
    "    ece_score = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        if np.any(bin_mask):\n",
    "            bin_true = y_true[bin_mask]\n",
    "            bin_pred = y_pred[bin_mask]\n",
    "            bin_acc = np.mean(bin_true)\n",
    "            bin_conf = np.mean(bin_pred)\n",
    "            ece_score += np.abs(bin_acc - bin_conf) * len(bin_true) / len(y_true)\n",
    "    return ece_score\n",
    "\n",
    "\n",
    "\n",
    "def nlpd(y_true, mu, sigma):\n",
    "    return -np.mean(0.5 * np.log(2 * np.pi * sigma**2) + 0.5 * ((y_true - mu) / sigma)**2)\n",
    "\n",
    "# -------------------------GMR Prediction Function-------------------------------------#\n",
    "def predict_with_covariance(gmm, in_idx, X, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    For each sample in X (shape [n_samples, n_features]),\n",
    "    compute the conditional mean and variance for the output,\n",
    "    which is assumed to be at index len(in_idx) in the joint data.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_components = gmm.n_components\n",
    "    out_idx = [len(in_idx)]  # output is at position equal to number of input features\n",
    "\n",
    "    pred_means = np.zeros(n_samples)\n",
    "    pred_vars = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        x = X[i]  # Input vector for sample i\n",
    "        comp_means = np.zeros(n_components)  # conditional mean per component\n",
    "        comp_vars = np.zeros(n_components)   # conditional variance per component\n",
    "        likelihoods = np.zeros(n_components) # likelihood for each component\n",
    "\n",
    "        for k in range(n_components):\n",
    "            mu = gmm.means[k]          # full mean for component k\n",
    "            Sigma = gmm.covariances[k] # full covariance for component k\n",
    "\n",
    "            # Partition into input and output parts:\n",
    "            mu_x = mu[in_idx]\n",
    "            mu_y = mu[out_idx][0]  # scalar output mean for component k\n",
    "            Sigma_xx = Sigma[np.ix_(in_idx, in_idx)]\n",
    "            Sigma_xy = Sigma[np.ix_(in_idx, out_idx)]\n",
    "            Sigma_yx = Sigma[np.ix_(out_idx, in_idx)]\n",
    "            Sigma_yy = Sigma[np.ix_(out_idx, out_idx)][0, 0]\n",
    "\n",
    "            # Regularize Sigma_xx and compute its inverse\n",
    "            Sigma_xx_reg = Sigma_xx + epsilon * np.eye(Sigma_xx.shape[0])\n",
    "            inv_Sigma_xx = np.linalg.inv(Sigma_xx_reg)\n",
    "\n",
    "            # Conditional mean and variance formulas:\n",
    "            cond_mean = mu_y + Sigma_yx.dot(inv_Sigma_xx).dot(x - mu_x)\n",
    "            cond_var = Sigma_yy - Sigma_yx.dot(inv_Sigma_xx).dot(Sigma_xy)\n",
    "            # Set a per-component variance floor to avoid overconfident predictions.\n",
    "            cond_var = max(cond_var, 0)\n",
    "\n",
    "            comp_means[k] = cond_mean\n",
    "            comp_vars[k] = cond_var\n",
    "\n",
    "            # Retrieve component weight\n",
    "            if hasattr(gmm, 'weights'):\n",
    "                weight_k = gmm.weights[k]\n",
    "            elif hasattr(gmm, 'priors'):\n",
    "                weight_k = gmm.priors[k]\n",
    "            else:\n",
    "                raise AttributeError(\"GMM object has no attribute 'weights' or 'priors'.\")\n",
    "                \n",
    "            likelihoods[k] = weight_k * multivariate_normal.pdf(x, mean=mu_x, cov=Sigma_xx_reg)\n",
    "\n",
    "        total_likelihood = np.sum(likelihoods)\n",
    "        if total_likelihood == 0:\n",
    "            resp = np.ones(n_components) / n_components\n",
    "        else:\n",
    "            resp = likelihoods / total_likelihood\n",
    "\n",
    "        overall_mean = np.sum(resp * comp_means)\n",
    "        overall_var = np.sum(resp * (comp_vars + (comp_means - overall_mean)**2))\n",
    "\n",
    "        pred_means[i] = overall_mean\n",
    "        pred_vars[i] = overall_var\n",
    "    return pred_means, pred_vars\n",
    "\n",
    "# -------------------------Define RFF Layer-------------------------------------#\n",
    "class RFFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, S, kernel_widths, feature_count, random_state=42, **kwargs):\n",
    "        super(RFFLayer, self).__init__(**kwargs)\n",
    "        self.S = S\n",
    "        self.feature_count = feature_count\n",
    "        self.kernel_widths = tf.constant(kernel_widths, dtype=tf.float32)\n",
    "        np.random.seed(random_state)\n",
    "        self.Z = [tf.constant(np.random.normal(0, 1, (S, 1)), dtype=tf.float32)\n",
    "                  for _ in range(feature_count)]\n",
    "        self.c = tf.constant(np.random.uniform(0, 2 * np.pi, S), dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        transformed_features = []\n",
    "        for i in range(self.feature_count):\n",
    "            feature = tf.expand_dims(inputs[:, i], axis=1)\n",
    "            prod = feature * tf.transpose(self.Z[i])\n",
    "            prod = prod / self.kernel_widths[i]\n",
    "            prod = prod + self.c\n",
    "            transformed = tf.sqrt(2.0 / self.S) * tf.cos(prod)\n",
    "            transformed_features.append(transformed)\n",
    "        bias = tf.ones((tf.shape(inputs)[0], 1))\n",
    "        return tf.concat([bias] + transformed_features, axis=1)\n",
    "\n",
    "# -------------------------Load Dataset-------------------------------------#\n",
    "# For this example, we use the Auto MPG dataset.\n",
    "data = pd.read_csv(\"/home/visdom/Downloads/auto-mpg.csv\")\n",
    "data['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')\n",
    "data = data.dropna(subset=['horsepower'])\n",
    "X = data.drop(columns=['mpg']).copy()\n",
    "y = data['mpg'].values\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    X = pd.get_dummies(X, columns=categorical_columns)\n",
    "X = X.values\n",
    "\n",
    "# Set GPFlow default float type\n",
    "gpf.config.set_default_float(tf.float32)\n",
    "\n",
    "# -------------------------Experiment Loop-------------------------------------#\n",
    "num_experiments = 10\n",
    "rmse_list = []\n",
    "ece_list = []\n",
    "nlpd_list = []\n",
    "\n",
    "# Training parameters\n",
    "S = 100\n",
    "epochs = 300\n",
    "learning_rate = 0.005\n",
    "\n",
    "for run in range(num_experiments):\n",
    "    print(f\"\\n=== Experiment {run+1}/{num_experiments} ===\")\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=run)\n",
    "    \n",
    "    # Scale features and targets to [0,1]\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    feature_count = X_train_scaled.shape[1]\n",
    "    kernel_widths = np.random.uniform(0.1, 2.0, feature_count)\n",
    "    \n",
    "    # Initialize RFF layer and base model weights (fresh model each run)\n",
    "    rff_layer = RFFLayer(S, kernel_widths, feature_count, random_state=run)\n",
    "    rff_output_dim = 1 + feature_count * S\n",
    "    w = tf.Variable(tf.zeros((rff_output_dim, 1)), dtype=tf.float32)\n",
    "    \n",
    "    def base_prediction(X_input):\n",
    "        phi = rff_layer(X_input)\n",
    "        return tf.matmul(phi, w)\n",
    "    \n",
    "    def base_loss(X_input, y_true):\n",
    "        y_pred = base_prediction(X_input)\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        reg = tf.nn.l2_loss(w)\n",
    "        return 0.5 * mse + 0.5 * reg\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Train the base model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_val = base_loss(tf.convert_to_tensor(X_train_scaled, dtype=tf.float32),\n",
    "                                 tf.convert_to_tensor(y_train_scaled, dtype=tf.float32))\n",
    "        gradients = tape.gradient(loss_val, [w] + rff_layer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, [w] + rff_layer.trainable_variables))\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Base Loss = {loss_val.numpy():.4f}\")\n",
    "    \n",
    "    # Compute residuals on training data\n",
    "    X_train_tf = tf.convert_to_tensor(X_train_scaled, dtype=tf.float32)\n",
    "    y_train_pred = base_prediction(X_train_tf).numpy()\n",
    "    residuals_train = y_train_scaled - y_train_pred  # shape (n_samples, 1)\n",
    "    \n",
    "    # Train GMR on residuals (concatenate X and residual)\n",
    "    train_data_gmr = np.hstack((X_train_scaled, residuals_train))\n",
    "    \n",
    "    # Select the optimal number of GMM components using BIC\n",
    "    n_components_range = range(1, 11)\n",
    "    bic_scores = []\n",
    "    for n in n_components_range:\n",
    "        gm = GaussianMixture(n_components=n, covariance_type='full', random_state=run)\n",
    "        gm.fit(train_data_gmr)\n",
    "        bic_scores.append(gm.bic(train_data_gmr))\n",
    "    optimal_n = n_components_range[np.argmin(bic_scores)]\n",
    "    print(\"Optimal number of GMM components:\", optimal_n)\n",
    "    \n",
    "    # Fit the GMR model using the gmr library\n",
    "    gmr_model = GMM(n_components=optimal_n, random_state=run)\n",
    "    gmr_model.from_samples(train_data_gmr)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    X_test_tf = tf.convert_to_tensor(X_test_scaled, dtype=tf.float32)\n",
    "    y_base_test = base_prediction(X_test_tf).numpy()\n",
    "    \n",
    "    # Use GMR to predict the residual correction\n",
    "    input_indices = list(range(feature_count))\n",
    "    predicted_residuals, predicted_vars = predict_with_covariance(gmr_model, input_indices, X_test_scaled)\n",
    "    \n",
    "    # Compute overall predicted standard deviation and enforce a global floor\n",
    "    min_std = 0\n",
    "    predicted_std = np.maximum(np.sqrt(predicted_vars), min_std)\n",
    "    \n",
    "    # Final prediction: base prediction + residual correction\n",
    "    y_pred_test = y_base_test.flatten() + predicted_residuals\n",
    "    \n",
    "    # Evaluate metrics on scaled targets\n",
    "    run_rmse = rmse(tf.convert_to_tensor(y_test_scaled, dtype=tf.float32), \n",
    "                    tf.convert_to_tensor(y_pred_test.reshape(-1,1), dtype=tf.float32))\n",
    "    run_ece = ece(y_test_scaled, y_pred_test)\n",
    "    run_nlpd = nlpd(y_test_scaled, y_pred_test, predicted_std)\n",
    "    \n",
    "    print(f\"Run {run+1} - RMSE: {run_rmse:.4f}, ECE: {run_ece:.4f}, NLPD: {run_nlpd:.4f}\")\n",
    "    \n",
    "    rmse_list.append(run_rmse)\n",
    "    ece_list.append(run_ece)\n",
    "    nlpd_list.append(run_nlpd)\n",
    "\n",
    "# Report mean and standard deviation of metrics over experiments\n",
    "print(\"\\n=== Final Results over 10 Experiments ===\")\n",
    "print(f\"Test RMSE: Mean = {np.mean(rmse_list):.4f}, Std = {np.std(rmse_list):.4f}\")\n",
    "print(f\"Test ECE:  Mean = {np.mean(ece_list):.4f}, Std = {np.std(ece_list):.4f}\")\n",
    "print(f\"Test NLPD: Mean = {np.mean(nlpd_list):.4f}, Std = {np.std(nlpd_list):.4f}\")\n",
    "\n",
    "# Verify that no GPUs are used\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
